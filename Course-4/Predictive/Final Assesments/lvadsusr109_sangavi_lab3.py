# -*- coding: utf-8 -*-
"""LVADSUSR109_Sangavi_Lab3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UWziFOgf3rY9_YbueReGeeWIqO18dLDh
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score,LeaveOneOut
import seaborn as sns
from sklearn.metrics import confusion_matrix , classification_report
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import LeavePOut, cross_val_score
from sklearn.model_selection import ShuffleSplit, cross_val_score

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv("/content/drive/MyDrive/Predictive/Final/seeds.csv")

print(df.head())

#EDA
df.info()
df.describe()

# Performing PCA for dimension Reduction
'''
PCA
'''
dfa=df
np.random.seed(42)

pca = PCA()
df_pca = pca.fit_transform(dfa)

# Plot Explained Variance Ratio
explained_var_ratio = pca.explained_variance_ratio_
cumulative_var_ratio = np.cumsum(explained_var_ratio)

plt.plot(range(1, len(cumulative_var_ratio) + 1), cumulative_var_ratio, marker='o')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.title('Explained Variance Ratio vs. Number of Principal Components')
plt.show()

# check for missing values
print(df.isnull().sum())
# Droping the record since the missing value is 1
df=df.dropna()
# Check for null after filling the missing value
# We can fill the missing value using fillna()
print(df.isnull().sum())

# Check for duplicate records
print(df.duplicated().sum()) # No duplicate records
# No categorical value for encoding

print(df)
X=df.drop(columns=['Area','Perimeter','Compactness'])
Y=df['Asymmetry coefficient']

# Using Elbow method to find the number of clusters that are optimal
sse = [] # The sum of Squared Errors =SSE
k_rng = range(1,10)
for k in k_rng:
    km = KMeans(n_clusters=k)
    km.fit(df)
    sse.append(km.inertia_)

plt.xlabel('K')
plt.ylabel('Sum of squared error')
plt.plot(k_rng,sse)

n_clusters = 3  # Adjust this number based on the elbow curve and your requirements
km = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)
km.fit(df)
y_predicted = km.fit_predict(df)
df['cluster'] = y_predicted
df.head(25)
LE_data_cluster = df.sort_values(by=['cluster'])
LE_data_cluster.head()
pca = PCA(n_components=2).fit(df)
LE_clustered_2d = pca.transform(df)
plt.figure('QoL Combined Cluster Plot')
plt.scatter(LE_clustered_2d[:,0],LE_clustered_2d[:,1],c=km.labels_)
plt.show
LE_data_cluster.head()

# @title Area vs Perimeter

from matplotlib import pyplot as plt
LE_data_cluster.plot(kind='scatter', x='Area', y='Perimeter', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

df1 = df[df.cluster==0]
df2 = df[df.cluster==1]
df3 = df[df.cluster==2]

#Cluster heads
plt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='black',marker='o',label='centroid')